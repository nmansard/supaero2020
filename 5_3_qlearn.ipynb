{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving optimal control using Q-learning \n",
    "This notebook introduces the algorithm of deep Q-learning, for a discrete pendulum.\n",
    "\n",
    "## Setup\n",
    "For the notebook, we need tensorflow:\n",
    "\n",
    "- pip install --user --upgrade pip\n",
    "\n",
    "- pip install --user tensorflow==2.0.0\n",
    "\n",
    "We will use a simple pendulum model with a single DOF. It is wrapped in the file tp5.env_pendulum. Three different versions are available: discrete state and discrete control in EnvPendulumDiscrete ; continuous state and discrete control in EnvPendulumHybrid ; and continuous state and control space in EnvPendulum.\n",
    "Don't forget to run gepetto-viewer first is you want to display the model. The environments follows the API introduced in \"Gym @ OpenIA\": \n",
    "- env.reset() internally set the environment state.\n",
    "- env.step() change the internal state and returns a pair (next_state,reward)\n",
    "- env.render() display the internal state.\n",
    "\n",
    "We also need a bunch of classic python stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Table\n",
    "The first algorithm solves the discrete problem by representing the Q function Q(x,u) = l(x,u) + V(f(x,u)) as a table, and search the optimal Q function by enforcing the HJB equation. The policy is simply obtained as the argmax of the Q-table.\n",
    "\n",
    "For this algorithm, we need the discrete pendulum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 18-19 tp5/qtable.py\n",
    "from tp5.env_pendulum import EnvPendulumDiscrete; Env = lambda : EnvPendulumDiscrete(1)\n",
    "env = Env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters are relatively few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 22-25 tp5/qtable.py\n",
    "NEPISODES               = 200           # Number of training episodes\n",
    "NSTEPS                  = 50            # Max episode length\n",
    "LEARNING_RATE           = 0.85          # \n",
    "DECAY_RATE              = 0.99          # Discount factor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q function is represented by a table. Evaluating Q(x,u) is as simple as choosing the coefficient x,u in the table Q[x,u]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 27 tp5/qtable.py\n",
    "Q     = np.zeros([env.nx,env.nu])       # Q-table initialized to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm runs a number of training episode NEPISODES. At each step of an episod, the Q table is adjusted to better match HJB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 39-57 tp5/qtable.py\n",
    "h_rwd = []                              # Learning history (for plot).\n",
    "for episode in range(1,NEPISODES):\n",
    "    x    = env.reset()\n",
    "    rsum = 0.0\n",
    "    for steps in range(NSTEPS):\n",
    "        u         = np.argmax(Q[x,:] + np.random.randn(1,env.nu)/episode) # Greedy action with noise\n",
    "        x2,reward = env.step(u)\n",
    "        \n",
    "        # Compute reference Q-value at state x respecting HJB\n",
    "        Qref = reward + DECAY_RATE*np.max(Q[x2,:])\n",
    "\n",
    "        # Update Q-Table to better fit HJB\n",
    "        Q[x,u] += LEARNING_RATE*(Qref-Q[x,u])\n",
    "        x       = x2\n",
    "        rsum   += reward\n",
    "\n",
    "    h_rwd.append(rsum)\n",
    "    if not episode%20:\n",
    "        print('Episode #%d done with average cost %.2f' % (episode,sum(h_rwd[-20:])/20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try a roll-out with the following \"rendertrial\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 29-35 tp5/qtable.py\n",
    "def rendertrial(s0=None,maxiter=100):\n",
    "    '''Roll-out from random state using greedy policy.'''\n",
    "    s = env.reset(s0)\n",
    "    for i in range(maxiter):\n",
    "        a = np.argmax(Q[s,:])\n",
    "        s,r = env.step(a)\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendertrial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Q-table algorithm as a neural algorithm\n",
    "As an intermediate step before introducing the deep-Q algorithm, we suggest a modification of the Q-table algorithm by representing the Q-table as a neural network. To make it simple, the neural network is chosen with a trivial structure: a single layer, linear activation (i.e. it is a matrix!).\n",
    "Then, the Q-table update becomes a gradient descent, which is less efficient but more generic and would scale adequately if we change the network structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed = 7943\n",
      "WARNING:tensorflow:From /home/nmansard/.local/lib/python3.5/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Episode #20 done with 0 sucess\n",
      "Episode #40 done with 0 sucess\n",
      "Episode #60 done with 0 sucess\n",
      "Episode #80 done with 0 sucess\n",
      "Episode #100 done with 0 sucess\n",
      "Episode #120 done with 0 sucess\n",
      "Episode #140 done with 0 sucess\n",
      "Episode #160 done with 0 sucess\n",
      "Episode #180 done with 0 sucess\n",
      "Episode #200 done with 0 sucess\n",
      "Episode #220 done with 0 sucess\n",
      "Episode #240 done with 0 sucess\n",
      "Episode #260 done with 0 sucess\n",
      "Episode #280 done with 0 sucess\n",
      "Episode #300 done with 0 sucess\n",
      "Episode #320 done with 0 sucess\n",
      "Episode #340 done with 0 sucess\n",
      "Episode #360 done with 0 sucess\n",
      "Episode #380 done with 0 sucess\n",
      "Episode #400 done with 0 sucess\n",
      "Episode #420 done with 0 sucess\n",
      "Episode #440 done with 0 sucess\n",
      "Episode #460 done with 0 sucess\n",
      "Episode #480 done with 0 sucess\n",
      "Total rate of success: 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load tp5/deeptable.py\n",
    "'''\n",
    "Example of Q-table learning with a simple discretized 1-pendulum environment using a linear Q network.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf1\n",
    "import matplotlib.pyplot as plt\n",
    "from tp5.env_pendulum import EnvPendulumDiscrete; Env = lambda : EnvPendulumDiscrete(1)\n",
    "import signal\n",
    "import time\n",
    "tf1.disable_eager_execution()\n",
    "\n",
    "\n",
    "### --- Random seed\n",
    "RANDOM_SEED = int((time.time()%10)*1000)\n",
    "print(\"Seed = %d\" % RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "### --- Hyper paramaters\n",
    "NEPISODES               = 500           # Number of training episodes\n",
    "NSTEPS                  = 50            # Max episode length\n",
    "LEARNING_RATE           = 0.1           # Step length in optimizer\n",
    "DECAY_RATE              = 0.99          # Discount factor \n",
    "\n",
    "### --- Environment\n",
    "env = Env()\n",
    "NX  = env.nx\n",
    "NU  = env.nu\n",
    "\n",
    "### --- Q-value networks\n",
    "class QValueNetwork:\n",
    "    def __init__(self):\n",
    "        x               = tf1.placeholder(shape=[1,NX],dtype=tf.float32)\n",
    "        W               = tf1.Variable(tf1.random_uniform([NX,NU],0,0.01,seed=100))\n",
    "        qvalue          = tf1.matmul(x,W)\n",
    "        u               = tf1.argmax(qvalue,1)\n",
    "\n",
    "        qref            = tf1.placeholder(shape=[1,NU],dtype=tf.float32)\n",
    "        loss            = tf1.reduce_sum(tf.square(qref - qvalue))\n",
    "        optim           = tf1.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "        self.x          = x             # Network input\n",
    "        self.qvalue     = qvalue        # Q-value as a function of x\n",
    "        self.u          = u             # Policy  as a function of x\n",
    "        self.qref       = qref          # Reference Q-value at next step (to be set to l+Q o f)\n",
    "        self.optim      = optim         # Optimizer      \n",
    "\n",
    "### --- Tensor flow initialization\n",
    "#tf.reset_default_graph()\n",
    "qvalue  = QValueNetwork()\n",
    "sess = tf1.InteractiveSession()\n",
    "tf1.global_variables_initializer().run()\n",
    "\n",
    "def onehot(ix,n=NX):\n",
    "    '''Return a vector which is 0 everywhere except index <i> set to 1.'''\n",
    "    return np.array([[ (i==ix) for i in range(n) ],],np.float)\n",
    "   \n",
    "def disturb(u,i):\n",
    "    u += int(np.random.randn()*10/(i/50+10))\n",
    "    return np.clip(u,0,NU-1)\n",
    "\n",
    "def rendertrial(maxiter=100):\n",
    "    x = env.reset()\n",
    "    for i in range(maxiter):\n",
    "        u = sess.run(qvalue.u,feed_dict={ qvalue.x:onehot(x) })\n",
    "        x,r = env.step(u)\n",
    "        env.render()\n",
    "        if r==1: print('Reward!'); break\n",
    "signal.signal(signal.SIGTSTP, lambda x,y:rendertrial()) # Roll-out when CTRL-Z is pressed\n",
    "\n",
    "### --- History of search\n",
    "h_rwd = []                              # Learning history (for plot).\n",
    "\n",
    "### --- Training\n",
    "for episode in range(1,NEPISODES):\n",
    "    x    = env.reset()\n",
    "    rsum = 0.0\n",
    "\n",
    "    for step in range(NSTEPS-1):\n",
    "        u = sess.run(qvalue.u,feed_dict={ qvalue.x: onehot(x) })[0] # Greedy policy ...\n",
    "        u = disturb(u,episode)                                      # ... with noise\n",
    "        x2,reward = env.step(u)\n",
    "\n",
    "        # Compute reference Q-value at state x respecting HJB\n",
    "        Q2        = sess.run(qvalue.qvalue,feed_dict={ qvalue.x: onehot(x2) })\n",
    "        Qref      = sess.run(qvalue.qvalue,feed_dict={ qvalue.x: onehot(x ) })\n",
    "        Qref[0,u] = reward + DECAY_RATE*np.max(Q2)\n",
    "\n",
    "        # Update Q-table to better fit HJB\n",
    "        sess.run(qvalue.optim,feed_dict={ qvalue.x    : onehot(x),\n",
    "                                          qvalue.qref : Qref       })\n",
    "\n",
    "        rsum += reward\n",
    "        x = x2\n",
    "        if reward == 1: break\n",
    "\n",
    "    h_rwd.append(rsum)\n",
    "    if not episode%20: print('Episode #%d done with %d sucess' % (episode,sum(h_rwd[-20:])))\n",
    "\n",
    "print(\"Total rate of success: %.3f\" % (sum(h_rwd)/NEPISODES))\n",
    "rendertrial()\n",
    "plt.plot( np.cumsum(h_rwd)/range(1,NEPISODES) )\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep-Q algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deep-Q algorithm is a subtil variation of the previous algorithm. The main difference is that ... it works. For that, we need a little bit of magic:\n",
    "- we introduce a \"target\" network, which somehow smooth the excessive variations of the network weights introduced by the gradient descent.\n",
    "- we keep the recent previous experiences in a memory buffer, that breaks the temporal structure when learning the Q function.\n",
    "\n",
    "Now that we have a proper neural network, we can work with a continuous state. The first layers of the network will learn a proper representation of the state, no need to discretize it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tp5.env_pendulum import EnvPendulumHybrid\n",
    "env = EnvPendulumHybrid(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some more hyper parameters to tune the magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 32-40 tp5/qlearn.py\n",
    "### --- Hyper paramaters\n",
    "NEPISODES               = 1000          # Max training steps\n",
    "NSTEPS                  = 60            # Max episode length\n",
    "QVALUE_LEARNING_RATE    = 0.001         # Base learning rate for the Q-value Network\n",
    "DECAY_RATE              = 0.99          # Discount factor \n",
    "UPDATE_RATE             = 0.01          # Homotopy rate to update the networks\n",
    "REPLAY_SIZE             = 10000         # Size of replay buffer\n",
    "BATCH_SIZE              = 64            # Number of points to be fed in stochastic gradient\n",
    "NH1 = NH2               = 32            # Hidden layer size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a replay buffer to store the past experience. We store that in a \"deque\" where data can be added and removed from both side of the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 42-51 tp5/qlearn.py\n",
    "### --- Replay memory\n",
    "class ReplayItem:\n",
    "    def __init__(self,x,u,r,d,x2):\n",
    "        self.x          = x\n",
    "        self.u          = u\n",
    "        self.reward     = r\n",
    "        self.done       = d\n",
    "        self.x2         = x2\n",
    "replayDeque = deque()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the code more readable, the network itself is built in a dedicated class that we do not exhibit here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tp5.qnetwork import QNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 52-57 tp5/qlearn.py\n",
    "### --- Tensor flow initialization\n",
    "qvalue          = QNetwork(nx=env.nx,nu=env.nu,learning_rate=QVALUE_LEARNING_RATE)\n",
    "qvalueTarget    = QNetwork(name='target',nx=env.nx,nu=env.nu)\n",
    "# Uncomment to load networks\n",
    "#qvalue.load()\n",
    "#qvalueTarget.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run the episodes. At each step of each episode, we store the result of the experience in the memory. If there is enough material in the memory, we discard the oldest experiences. Then we run one step of gradient descent to adjust the Q-function to HJB equality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 71-119 tp5/qlearn.py\n",
    "### History of search\n",
    "h_rwd = []\n",
    "\n",
    "### --- Training\n",
    "for episode in range(1,NEPISODES):\n",
    "    x    = env.reset()\n",
    "    rsum = 0.0\n",
    "\n",
    "    for step in range(NSTEPS):\n",
    "        u       = qvalue.policy(x,                                     # Greedy policy ...\n",
    "                                noise=1. / (1. + episode + step))      # ... with noise\n",
    "        x2,r    = env.step(u)\n",
    "        done    = False # Some environment may return information when task completed\n",
    "\n",
    "        replayDeque.append(ReplayItem(x,u,r,done,x2))                # Feed replay memory ...\n",
    "        if len(replayDeque)>REPLAY_SIZE: replayDeque.popleft()       # ... with FIFO forgetting.\n",
    "\n",
    "        rsum   += r\n",
    "        x       = x2\n",
    "        if done: break\n",
    "        \n",
    "        # Start optimizing networks when memory size > batch size.\n",
    "        if len(replayDeque) > BATCH_SIZE:     \n",
    "            batch = random.sample(replayDeque,BATCH_SIZE)            # Random batch from replay memory.\n",
    "            x_batch    = np.vstack([ b.x      for b in batch ])\n",
    "            u_batch    = np.vstack([ b.u      for b in batch ])\n",
    "            r_batch    = np.array([ [b.reward] for b in batch ])\n",
    "            d_batch    = np.array([ [b.done]   for b in batch ])\n",
    "            x2_batch   = np.vstack([ b.x2     for b in batch ])\n",
    "            \n",
    "            # Compute Q(x,u) from target network\n",
    "            v_batch    = qvalueTarget.value(x2_batch)\n",
    "            qref_batch = r_batch + (d_batch==False)*(DECAY_RATE*v_batch)\n",
    "\n",
    "            # Update qvalue to solve HJB constraint: q = r + q'\n",
    "            qvalue.trainer.train_on_batch([x_batch,u_batch],qref_batch)\n",
    "            \n",
    "            # Update target networks by homotopy.\n",
    "            qvalueTarget.targetAssign(qvalue,UPDATE_RATE)\n",
    "      \n",
    "    # \\\\\\END_FOR step in range(NSTEPS)\n",
    "\n",
    "    # Display and logging (not mandatory).\n",
    "    print('Ep#{:3d}: lasted {:d} steps, reward={:3.0f}' .format(episode, step,rsum))\n",
    "    h_rwd.append(rsum)\n",
    "    if not (episode+1) % 200:     rendertrial(30)\n",
    "\n",
    "# \\\\\\END_FOR episode in range(NEPISODES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously, we can run the policy with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 59-68 tp5/qlearn.py\n",
    "def rendertrial(maxiter=NSTEPS,verbose=True):\n",
    "    x = env.reset()\n",
    "    rsum = 0.\n",
    "    for i in range(maxiter):\n",
    "        u = qvalue.policy(x)[0]\n",
    "        x, reward = env.step(u)\n",
    "        env.render()\n",
    "        time.sleep(1e-2)\n",
    "        rsum += reward\n",
    "    if verbose: print('Lasted ',i,' timestep -- total reward:',rsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendertrial()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
